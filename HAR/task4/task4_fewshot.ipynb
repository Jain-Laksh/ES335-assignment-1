{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\LAKSH_LAYING.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\LAKSH_SITTING.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\LAKSH_STANDING.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\LAKSH_WALKING.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\LAKSH_WALKING_DOWNSTAIRS.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\LAKSH_WALKING_UPSTAIRS.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\RUDRA_LAYING.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\RUDRA_SITTING.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\RUDRA_STANDING.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\RUDRA_WALKING.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\RUDRA_WALKING_DOWNSTAIRS.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\RUDRA_WALKING_UPSTAIRS.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\PARTHIV_LAYING.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\PARTHIV_SITTING.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\PARTHIV_STANDING.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\PARTHIV_WALKING.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\PARTHIV_WALKING_DOWNSTAIRS.csv\n",
      "Checking file: c:\\Users\\Parth\\OneDrive - iitgn.ac.in\\Codes\\ML\\ES335-assignment-1\\HAR\\task4\\PARTHIV_WALKING_UPSTAIRS.csv\n",
      "Data merging complete. The combined dataset (first 500 rows) has been created and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define subjects\n",
    "subjects = ['LAKSH', 'RUDRA', 'PARTHIV']\n",
    "\n",
    "# Define activities\n",
    "activities = ['LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS', 'WALKING_UPSTAIRS']\n",
    "\n",
    "# Function to load the first 500 rows of each dataset\n",
    "def load_data(subjects, activities):\n",
    "    data_list = []\n",
    "    for subject in subjects:\n",
    "        for activity in activities:\n",
    "            file_name = f\"{subject}_{activity}.csv\"\n",
    "            file_path = os.path.join(os.getcwd(), file_name)\n",
    "            print(f\"Checking file: {file_path}\")  # Debugging line\n",
    "            if os.path.exists(file_path):\n",
    "                # Read the first 500 rows\n",
    "                df = pd.read_csv(file_path, nrows=500)\n",
    "                df['Activity'] = activity\n",
    "                df['Subject'] = subject\n",
    "                data_list.append(df)\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "    return pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "# Load and process data\n",
    "combined_data_df = load_data(subjects, activities)\n",
    "\n",
    "# Save combined DataFrame to CSV if needed\n",
    "combined_data_df.to_csv('combined_taken_data.csv', index=False)\n",
    "\n",
    "print(\"Data merging complete. The combined dataset (first 500 rows) has been created and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>gFx</th>\n",
       "      <th>gFy</th>\n",
       "      <th>gFz</th>\n",
       "      <th>TgF</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010562</td>\n",
       "      <td>0.3288</td>\n",
       "      <td>-0.3991</td>\n",
       "      <td>-0.8522</td>\n",
       "      <td>1.00</td>\n",
       "      <td>LAYING</td>\n",
       "      <td>LAKSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036925</td>\n",
       "      <td>0.3302</td>\n",
       "      <td>-0.4017</td>\n",
       "      <td>-0.8520</td>\n",
       "      <td>1.00</td>\n",
       "      <td>LAYING</td>\n",
       "      <td>LAKSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.042296</td>\n",
       "      <td>0.3319</td>\n",
       "      <td>-0.4014</td>\n",
       "      <td>-0.8506</td>\n",
       "      <td>1.00</td>\n",
       "      <td>LAYING</td>\n",
       "      <td>LAKSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058767</td>\n",
       "      <td>0.3346</td>\n",
       "      <td>-0.4027</td>\n",
       "      <td>-0.8482</td>\n",
       "      <td>1.00</td>\n",
       "      <td>LAYING</td>\n",
       "      <td>LAKSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.092472</td>\n",
       "      <td>0.3361</td>\n",
       "      <td>-0.4061</td>\n",
       "      <td>-0.8498</td>\n",
       "      <td>1.00</td>\n",
       "      <td>LAYING</td>\n",
       "      <td>LAKSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>10.036258</td>\n",
       "      <td>0.6290</td>\n",
       "      <td>0.1049</td>\n",
       "      <td>0.2126</td>\n",
       "      <td>0.67</td>\n",
       "      <td>WALKING_UPSTAIRS</td>\n",
       "      <td>PARTHIV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>10.062572</td>\n",
       "      <td>0.6412</td>\n",
       "      <td>0.0711</td>\n",
       "      <td>0.2192</td>\n",
       "      <td>0.68</td>\n",
       "      <td>WALKING_UPSTAIRS</td>\n",
       "      <td>PARTHIV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>10.079894</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.2325</td>\n",
       "      <td>0.73</td>\n",
       "      <td>WALKING_UPSTAIRS</td>\n",
       "      <td>PARTHIV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>10.097549</td>\n",
       "      <td>0.7275</td>\n",
       "      <td>0.1801</td>\n",
       "      <td>0.2133</td>\n",
       "      <td>0.78</td>\n",
       "      <td>WALKING_UPSTAIRS</td>\n",
       "      <td>PARTHIV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>10.115518</td>\n",
       "      <td>0.7751</td>\n",
       "      <td>0.2434</td>\n",
       "      <td>0.1575</td>\n",
       "      <td>0.83</td>\n",
       "      <td>WALKING_UPSTAIRS</td>\n",
       "      <td>PARTHIV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           time     gFx     gFy     gFz   TgF          Activity  Subject\n",
       "0      0.010562  0.3288 -0.3991 -0.8522  1.00            LAYING    LAKSH\n",
       "1      0.036925  0.3302 -0.4017 -0.8520  1.00            LAYING    LAKSH\n",
       "2      0.042296  0.3319 -0.4014 -0.8506  1.00            LAYING    LAKSH\n",
       "3      0.058767  0.3346 -0.4027 -0.8482  1.00            LAYING    LAKSH\n",
       "4      0.092472  0.3361 -0.4061 -0.8498  1.00            LAYING    LAKSH\n",
       "...         ...     ...     ...     ...   ...               ...      ...\n",
       "8995  10.036258  0.6290  0.1049  0.2126  0.67  WALKING_UPSTAIRS  PARTHIV\n",
       "8996  10.062572  0.6412  0.0711  0.2192  0.68  WALKING_UPSTAIRS  PARTHIV\n",
       "8997  10.079894  0.6825  0.1017  0.2325  0.73  WALKING_UPSTAIRS  PARTHIV\n",
       "8998  10.097549  0.7275  0.1801  0.2133  0.78  WALKING_UPSTAIRS  PARTHIV\n",
       "8999  10.115518  0.7751  0.2434  0.1575  0.83  WALKING_UPSTAIRS  PARTHIV\n",
       "\n",
       "[9000 rows x 7 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.27777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "X = combined_data_df.iloc[:,0:-2]\n",
    "y = combined_data_df.iloc[:,-2]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_predict = dt.predict(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "accuracy = 0\n",
    "for i in range(len(y_test)):\n",
    "    if (y_test[i]==y_predict[i]):\n",
    "        accuracy +=1\n",
    "\n",
    "accuracy = accuracy*100/len(y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LAYING', 'LAYING', 'LAYING', ..., 'WALKING_UPSTAIRS',\n",
       "       'WALKING_UPSTAIRS', 'WALKING_UPSTAIRS'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth\\AppData\\Local\\Temp\\ipykernel_25116\\3076278180.py:20: UserWarning: Using default sampling frequency set in configuration file.\n",
      "  features = tsfel.time_series_features_extractor(cfg_file, X)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 100% Complete\n",
       "              <p/>\n",
       "              <progress\n",
       "                  value='67'\n",
       "                  max='67',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  67\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.444444444444445\n"
     ]
    }
   ],
   "source": [
    "import tsfel\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming X and y are already loaded as pandas DataFrames or NumPy arrays\n",
    "# X.shape should be (9000, 5)\n",
    "# y.shape should be (9000, 1)\n",
    "\n",
    "# If X and y are not DataFrames, convert them\n",
    "# if isinstance(X, np.ndarray):\n",
    "#     X = pd.DataFrame(X, columns=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5'])\n",
    "# if isinstance(y, np.ndarray):\n",
    "y = pd.DataFrame(y, columns=['Target'])\n",
    "# print(y)\n",
    "# X = X.iloc[:,1:]\n",
    "# Feature extraction with TSFEL\n",
    "cfg_file = tsfel.get_features_by_domain()  # Use default configuration file\n",
    "features = tsfel.time_series_features_extractor(cfg_file, X)\n",
    "\n",
    "# Concatenate the extracted features with the target variable\n",
    "features_with_target = pd.concat([features, y], axis=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_with_target.drop(columns='Target'), features_with_target['Target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "y_test = np.array(y_test)\n",
    "y_test\n",
    "\n",
    "accuracy = 0\n",
    "for i in range(len(y_test)):\n",
    "    if (y_test[i]==y_pred[i]):\n",
    "        accuracy +=1\n",
    "\n",
    "accuracy = accuracy*100/len(y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "model_name = os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}\n",
    "\n",
    "llm = ChatGroq(model=groq_models[model_name] , api_key=groq_api_key, temperature=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TRAINED ON GIVEN DATASET, TESTED ON COLLECTED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYING LAYING\n",
      "1\n",
      "SITTING STANDING\n",
      "STANDING STANDING\n",
      "2\n",
      "WALKING WALKING\n",
      "3\n",
      "WALKING_DOWNSTAIRS WALKING_UPSTAIRS\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "4\n",
      "SITTING STANDING\n",
      "STANDING STANDING\n",
      "5\n",
      "WALKING WALKING_UPSTAIRS\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING_UPSTAIRS\n",
      "6\n",
      "LAYING LAYING\n",
      "7\n",
      "SITTING STANDING\n",
      "STANDING STANDING\n",
      "8\n",
      "WALKING WALKING\n",
      "9\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "Classification complete. Results have been saved to 'model_predictions.csv'.\n",
      "accuracy 50.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the test data loaded as a DataFrame\n",
    "test_data_df = pd.read_csv('combined_taken_data.csv')\n",
    "\n",
    "# Placeholder for storing results\n",
    "results = []\n",
    "\n",
    "# Define the activities\n",
    "activities = ['LAYING', 'STANDING', 'WALKING', 'SITTING', 'WALKING UPSTAIRS', 'WALKING DOWNSTAIRS']\n",
    "\n",
    "def sample_data(data, sample_rate=10):\n",
    "    n=500//sample_rate\n",
    "    return data[100:n+100]\n",
    "\n",
    "i = 0\n",
    "\n",
    "# Filter training data for specific subjects\n",
    "train_subjects = [1, 3, 5, 7]\n",
    "train_data_df = pd.read_csv('train_data_combined.csv')\n",
    "train_data_df = train_data_df[train_data_df['Subject'].isin(train_subjects)]\n",
    "\n",
    "# Iterate through each group by subject and activity\n",
    "for (subject, activity), group in test_data_df.groupby(['Subject', 'Activity']):\n",
    "    # Sample data\n",
    "    accx = sample_data(group['gFx'].tolist())\n",
    "    accy = sample_data(group['gFy'].tolist())\n",
    "    accz = sample_data(group['gFz'].tolist())\n",
    "    \n",
    "    # Create prompt for the current group\n",
    "    query_few_shot_Task4 = f\"\"\"\n",
    "    * You are a highly accurate activity classification model.\n",
    "    * Your task is to classify human activities based on the given accelerometer data.\n",
    "    * The accelerometer data is provided as mean acceleration values in the x, y, and z directions.\n",
    "    * You are given data corresponding to six different activities.\n",
    "    * The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING UPSTAIRS, and WALKING DOWNSTAIRS.\"\"\"\n",
    "\n",
    "    for (train_subject, train_activity), grp in train_data_df.groupby(['Subject', 'Activity']):\n",
    "        query_few_shot_Task4 += \"\"\"Here are some examples of accelerometer data and their corresponding activities:\"\"\"\n",
    "\n",
    "        query_few_shot_Task4 += f\"\"\"\n",
    "        * Activity: {train_activity}\n",
    "          accx = {sample_data(grp['accx'].tolist(), 50)}\n",
    "          accy = {sample_data(grp['accy'].tolist(), 50)}\n",
    "          accz = {sample_data(grp['accz'].tolist(), 50)}\n",
    "        \"\"\"\n",
    "\n",
    "    query_few_shot_Task4 += f\"\"\"\n",
    "    * Analyze the accelerometer data and provide the most likely activity label for each case.\n",
    "    * PRINT ONLY A WORD WHICH IS THE PREDICTED ACTIVITY AND NOTHING ELSE NO CONTENT NO REASON JUST A PREDICTION\n",
    "\n",
    "    accx = {accx}\n",
    "    accy = {accy}\n",
    "    accz = {accz}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate model prediction (replace with actual model prediction code)\n",
    "    result = llm.invoke(query_few_shot_Task4)\n",
    "    \n",
    "    print(activity, str(result).split(\" \")[0][8:].strip(\"'\"))\n",
    "    if (activity.upper() ==str(result).split(\" \")[0][8:].strip(\"'\")):\n",
    "        i+=1\n",
    "        print(i)\n",
    "    results.append({'Subject': subject, 'Activity': activity, 'Prediction': str(result).split(\" \")[0][8:].strip(\"'\")})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to CSV if needed\n",
    "results_df.to_csv('model_predictions_fewshot_4.1.csv', index=False)\n",
    "\n",
    "print(\"Classification complete. Results have been saved to 'model_predictions.csv'.\")\n",
    "\n",
    "print(\"accuracy\",i/18*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACCURACY FOR FEW SHOT IS 50% FOR UCI-HAR TRAINED MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> FEW SHOT FOR TRAIN AND TEST BOTH ON THE COLLECTED DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYING LAYING\n",
      "1\n",
      "SITTING STANDING\n",
      "STANDING STANDING\n",
      "2\n",
      "WALKING WALKING_UPSTAIRS\n",
      "WALKING_DOWNSTAIRS WALKING_UPSTAIRS\n",
      "WALKING_UPSTAIRS WALKING_UPSTAIRS\n",
      "3\n",
      "LAYING LAYING\n",
      "4\n",
      "SITTING STANDING\n",
      "STANDING STANDING\n",
      "5\n",
      "WALKING WALKING_UPSTAIRS\n",
      "WALKING_DOWNSTAIRS WALKING_UPSTAIRS\n",
      "WALKING_UPSTAIRS WALKING_UPSTAIRS\n",
      "6\n",
      "Classification complete. Results have been saved to 'model_predictions.csv'.\n",
      "accuracy 50.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the test data loaded as a DataFrame\n",
    "whole_df = pd.read_csv('combined_taken_data.csv')\n",
    "\n",
    "# Placeholder for storing results\n",
    "results = []\n",
    "\n",
    "# Define the activities\n",
    "activities = ['LAYING', 'STANDING', 'WALKING', 'SITTING', 'WALKING UPSTAIRS', 'WALKING DOWNSTAIRS']\n",
    "\n",
    "def sample_data(data, sample_rate=10):\n",
    "    n=500//sample_rate\n",
    "    return data[100:n+100]\n",
    "\n",
    "i = 0\n",
    "\n",
    "# Filter training data for specific subjects\n",
    "\n",
    "train_data_df = whole_df[whole_df[\"Subject\"] == \"LAKSH\"]\n",
    "test_data_df = whole_df[whole_df[\"Subject\"] != \"LAKSH\"]\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through each group by subject and activity\n",
    "for (subject, activity), group in test_data_df.groupby(['Subject', 'Activity']):\n",
    "    # Sample data\n",
    "    accx = sample_data(group['gFx'].tolist())\n",
    "    accy = sample_data(group['gFy'].tolist())\n",
    "    accz = sample_data(group['gFz'].tolist())\n",
    "    \n",
    "    # Create prompt for the current group\n",
    "    query_few_shot_Task4 = f\"\"\"\n",
    "    * You are a highly accurate activity classification model.\n",
    "    * Your task is to classify human activities based on the given accelerometer data.\n",
    "    * The accelerometer data is provided as mean acceleration values in the x, y, and z directions.\n",
    "    * You are given data corresponding to six different activities.\n",
    "    * The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING UPSTAIRS, and WALKING DOWNSTAIRS.\"\"\"\n",
    "\n",
    "    for (train_subject, train_activity), grp in train_data_df.groupby(['Subject', 'Activity']):\n",
    "        query_few_shot_Task4 += \"\"\"Here are some examples of accelerometer data and their corresponding activities:\"\"\"\n",
    "\n",
    "        query_few_shot_Task4 += f\"\"\"\n",
    "        * Activity: {train_activity}\n",
    "          accx = {sample_data(grp['gFx'].tolist(), 50)}\n",
    "          accy = {sample_data(grp['gFy'].tolist(), 50)}\n",
    "          accz = {sample_data(grp['gFz'].tolist(), 50)}\n",
    "        \"\"\"\n",
    "    \n",
    "    query_few_shot_Task4 += f\"\"\"\n",
    "    * Analyze the accelerometer data and provide the most likely activity label for each case.\n",
    "    * PRINT ONLY A WORD WHICH IS THE PREDICTED ACTIVITY AND NOTHING ELSE NO CONTENT NO REASON JUST A PREDICTION\n",
    "\n",
    "    accx = {accx}\n",
    "    accy = {accy}\n",
    "    accz = {accz}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate model prediction (replace with actual model prediction code)\n",
    "    result = llm.invoke(query_few_shot_Task4)\n",
    "    \n",
    "    print(activity, str(result).split(\" \")[0][8:].strip(\"'\"))\n",
    "    if (activity.upper() ==str(result).split(\" \")[0][8:].strip(\"'\")):\n",
    "        i+=1\n",
    "        print(i)\n",
    "    results.append({'Subject': subject, 'Activity': activity, 'Prediction': str(result).split(\" \")[0][8:].strip(\"'\")})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to CSV if needed\n",
    "results_df.to_csv('model_predictions_fewshot_4.2.csv', index=False)\n",
    "\n",
    "print(\"Classification complete. Results have been saved to 'model_predictions.csv'.\")\n",
    "\n",
    "print(\"accuracy\",i/12*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACCURACY IS 50% FOR BOTH TEST AND TRAIN ON THE COLLECTED DATA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
