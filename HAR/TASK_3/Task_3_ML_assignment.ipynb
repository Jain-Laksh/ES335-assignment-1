{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_groq in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.9)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_groq) (0.9.0)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.26 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_groq) (0.2.33)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (0.1.99)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (23.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (8.2.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain_groq) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD THE HIDDEN API KEY AND THE MODEL NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "model_name = os.getenv(\"MODEL_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=groq_models[model_name] , api_key=groq_api_key, temperature=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Train_sub1_laying=pd.read_csv(r\"../Dataset/Combined/Train/LAYING/Subject_1.csv\")\n",
    "Train_sub3_sitting=pd.read_csv(r\"..\\Dataset\\Combined\\Train\\SITTING\\Subject_3.csv\")\n",
    "Train_sub5_standing=pd.read_csv(r\"..\\Dataset\\Combined\\Train\\Standing\\Subject_5.csv\")\n",
    "Train_sub7_walking=pd.read_csv(r\"..\\Dataset\\Combined\\Train\\LAYING\\Subject_7.csv\")\n",
    "Train_sub8_walking_downstairs=pd.read_csv(r\"..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_8.csv\")\n",
    "Train_sub11_walking_upstairs=pd.read_csv(r\"..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_11.csv\")\n",
    "\n",
    "\n",
    "Test_sub2_laying=pd.read_csv(r\"..\\Dataset\\Combined\\Test\\LAYING\\Subject_2.csv\")\n",
    "Test_sub9_sitting=pd.read_csv(r\"..\\Dataset\\Combined\\Test\\SITTING\\Subject_9.csv\")\n",
    "Test_sub10_standing=pd.read_csv(r\"..\\Dataset\\Combined\\Test\\Standing\\Subject_10.csv\")\n",
    "Test_sub4_walking=pd.read_csv(r\"..\\Dataset\\Combined\\Test\\LAYING\\Subject_4.csv\")\n",
    "Test_sub20_walking_downstairs=pd.read_csv(r\"..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_20.csv\")\n",
    "Test_sub24_walking_upstairs=pd.read_csv(r\"..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_24.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean calculations complete. Test and train mean datasets have been combined and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "base_path = r\"..\\Dataset\\Combined\"\n",
    "\n",
    "# Define test subjects\n",
    "test_subjects = {\n",
    "    'LAYING': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'SITTING': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'Standing': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'WALKING': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'WALKING_DOWNSTAIRS': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'WALKING_UPSTAIRS': [2, 4, 9, 10, 12, 13, 18, 20, 24]\n",
    "}\n",
    "\n",
    "# Define train subjects (excluding test subjects)\n",
    "train_subjects = {\n",
    "    'LAYING': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'SITTING': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'Standing': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'WALKING': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'WALKING_DOWNSTAIRS': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'WALKING_UPSTAIRS': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30]\n",
    "}\n",
    "\n",
    "# Function to calculate mean of acceleration values\n",
    "def load_and_mean(activity, subjects, data_type='Train'):\n",
    "    means = []\n",
    "    for subject in subjects:\n",
    "        file_path = os.path.join(base_path, data_type, activity, f'Subject_{subject}.csv')\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            mean_values = df[['accx', 'accy', 'accz']].mean()\n",
    "            mean_values['Activity'] = activity\n",
    "            mean_values['Subject'] = subject\n",
    "            means.append(mean_values)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    return pd.DataFrame(means)\n",
    "\n",
    "# Load and process test data\n",
    "test_means = {activity: load_and_mean(activity, subjects, 'Test') for activity, subjects in test_subjects.items()}\n",
    "\n",
    "# Load and process train data\n",
    "train_means = {activity: load_and_mean(activity, [sub for sub in train_subjects[activity] if sub not in test_subjects[activity]], 'Train') for activity in train_subjects}\n",
    "\n",
    "# Combine all activities for test and train mean data\n",
    "test_mean_df = pd.concat(test_means.values(), ignore_index=True)\n",
    "train_mean_df = pd.concat(train_means.values(), ignore_index=True)\n",
    "\n",
    "# Save combined mean DataFrames to CSV if needed\n",
    "test_mean_df.to_csv('test_data_means.csv', index=False)\n",
    "train_mean_df.to_csv('train_data_means.csv', index=False)\n",
    "\n",
    "print(\"Mean calculations complete. Test and train mean datasets have been combined and saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_2.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_4.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_9.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_10.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_12.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_13.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_18.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_20.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_24.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_2.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_4.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_9.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_10.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_12.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_13.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_18.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_20.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_24.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_2.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_4.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_9.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_10.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_12.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_13.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_18.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_20.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_24.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_2.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_4.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_9.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_10.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_12.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_13.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_18.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_20.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_24.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_2.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_4.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_9.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_10.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_12.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_13.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_18.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_20.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_24.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_2.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_4.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_9.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_10.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_12.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_13.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_18.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_20.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_24.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_1.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_3.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_5.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_6.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_7.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_8.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_11.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_14.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_15.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_16.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_17.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_19.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_21.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_22.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_23.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_25.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_26.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_27.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_28.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_29.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_30.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_1.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_3.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_5.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_6.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_7.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_8.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_11.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_14.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_15.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_16.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_17.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_19.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_21.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_22.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_23.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_25.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_26.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_27.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_28.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_29.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_30.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_1.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_3.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_5.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_6.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_7.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_8.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_11.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_14.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_15.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_16.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_17.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_19.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_21.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_22.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_23.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_25.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_26.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_27.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_28.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_29.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_30.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_1.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_3.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_5.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_6.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_7.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_8.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_11.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_14.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_15.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_16.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_17.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_19.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_21.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_22.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_23.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_25.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_26.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_27.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_28.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_29.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_30.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_1.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_3.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_5.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_6.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_7.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_8.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_11.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_14.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_15.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_16.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_17.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_19.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_21.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_22.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_23.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_25.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_26.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_27.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_28.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_29.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_30.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_1.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_3.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_5.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_6.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_7.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_8.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_11.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_14.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_15.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_16.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_17.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_19.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_21.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_22.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_23.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_25.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_26.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_27.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_28.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_29.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_30.csv\n",
      "Data merging complete. Test and train datasets (first 500 rows) have been combined and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "base_path = r\"..\\Dataset\\Combined\"\n",
    "\n",
    "# Define test subjects\n",
    "test_subjects = {\n",
    "    'LAYING': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'SITTING': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'Standing': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'WALKING': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'WALKING_DOWNSTAIRS': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'WALKING_UPSTAIRS': [2, 4, 9, 10, 12, 13, 18, 20, 24]\n",
    "}\n",
    "\n",
    "# Define train subjects (excluding test subjects)\n",
    "train_subjects = {\n",
    "    'LAYING': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'SITTING': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'Standing': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'WALKING': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'WALKING_DOWNSTAIRS': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'WALKING_UPSTAIRS': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30]\n",
    "}\n",
    "\n",
    "# Function to load the first 500 rows of each dataset\n",
    "def load_data(activity, subjects, data_type='Train'):\n",
    "    data_list = []\n",
    "    for subject in subjects:\n",
    "        file_path = os.path.join(base_path, data_type, activity, f'Subject_{subject}.csv')\n",
    "        print(f\"Checking file: {file_path}\")  # Debugging line\n",
    "        if os.path.exists(file_path):\n",
    "            # Read the first 500 rows\n",
    "            df = pd.read_csv(file_path, nrows=500)\n",
    "            df['Activity'] = activity\n",
    "            df['Subject'] = subject\n",
    "            data_list.append(df)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    return pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "# Load and process test data\n",
    "test_data = {activity: load_data(activity, subjects, 'Test') for activity, subjects in test_subjects.items()}\n",
    "\n",
    "# Load and process train data\n",
    "train_data = {activity: load_data(activity, [sub for sub in train_subjects[activity] if sub not in test_subjects[activity]], 'Train') for activity in train_subjects}\n",
    "\n",
    "# Combine all activities for test and train data\n",
    "test_data_df = pd.concat(test_data.values(), ignore_index=True)\n",
    "train_data_df = pd.concat(train_data.values(), ignore_index=True)\n",
    "\n",
    "# Save combined DataFrames to CSV if needed\n",
    "test_data_df.to_csv('test_data_combined.csv', index=False)\n",
    "train_data_df.to_csv('train_data_combined.csv', index=False)\n",
    "\n",
    "print(\"Data merging complete. Test and train datasets (first 500 rows) have been combined and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ZERO-SHOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CASE I \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only 6 test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot Classification: Based on the provided accelerometer data, I will predict the corresponding human activity labels. Here are the predictions:\n",
      "\n",
      "**Case 1:**\n",
      "accx = -0.023458, accy = 0.768965, accz = 0.630654\n",
      "**Predicted Activity:** STANDING\n",
      "\n",
      "**Case 2:**\n",
      "accx = 0.974312, accy = -0.031424, accz = 0.095099\n",
      "**Predicted Activity:** WALKING\n",
      "\n",
      "**Case 3:**\n",
      "accx = 1.010296, accy = -0.024385, accz = 0.028739\n",
      "**Predicted Activity:** WALKING\n",
      "\n",
      "**Case 4:**\n",
      "accx = 0.035202, accy = 0.933609, accz = 0.328544\n",
      "**Predicted Activity:** STANDING\n",
      "\n",
      "**Case 5:**\n",
      "accx = 0.987812, accy = -0.253559, accz = -0.030405\n",
      "**Predicted Activity:** WALKING DOWNWARDS\n",
      "\n",
      "**Case 6:**\n",
      "accx = 0.973563, accy = -0.222988, accz = -0.231508\n",
      "**Predicted Activity:** WALKING DOWNWARDS\n",
      "\n",
      "Please note that these predictions are based on the patterns and relationships learned from the accelerometer data and may not be 100% accurate.\n"
     ]
    }
   ],
   "source": [
    "query_zero_shot_1 = f\"\"\"\n",
    "* You are an activity classification model.\n",
    "* Your task is to predict the human activity based on the given accelerometer data.\n",
    "* The accelerometer data contains acceration in x , y , z respectfully.\n",
    "* Provide the predicted activity label based on the data.\n",
    "* You are given data of 6 different case.\n",
    "* classify them into LAYYING,STANDIN,WALKING,SITTING,WALKING UPWARDS,WALKING DOWNWARDS\n",
    "\n",
    "Accelerometer Data: {Test_sub2_laying.mean()},{Test_sub9_sitting.mean()},{Test_sub10_standing.mean()},{Test_sub4_walking.mean()},{Test_sub20_walking_downstairs.mean()}\n",
    ",{Test_sub24_walking_upstairs.mean()}\"\"\"\n",
    "\n",
    "response_zero_shot_1 = llm.invoke(query_zero_shot_1)\n",
    "print(\"Zero-Shot Classification:\", response_zero_shot_1.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33.33 percent are or 2/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CASE II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero shot for whole data with the average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot Classification: Based on the provided accelerometer data, I'll classify the activities from 0 to 53. Here are the results:\n",
      "\n",
      "0. WALKING\n",
      "1. WALKING\n",
      "2. WALKING\n",
      "3. WALKING\n",
      "4. WALKING\n",
      "5. WALKING\n",
      "6. WALKING\n",
      "7. WALKING\n",
      "8. WALKING\n",
      "9. WALKING\n",
      "10. WALKING\n",
      "11. WALKING\n",
      "12. WALKING\n",
      "13. WALKING\n",
      "14. WALKING\n",
      "15. WALKING\n",
      "16. WALKING\n",
      "17. WALKING\n",
      "18. WALKING\n",
      "19. WALKING\n",
      "20. WALKING\n",
      "21. WALKING\n",
      "22. WALKING\n",
      "23. WALKING\n",
      "24. WALKING\n",
      "25. WALKING\n",
      "26. WALKING\n",
      "27. WALKING\n",
      "28. WALKING\n",
      "29. WALKING\n",
      "30. WALKING\n",
      "31. WALKING\n",
      "32. WALKING\n",
      "33. WALKING\n",
      "34. WALKING\n",
      "35. WALKING\n",
      "36. WALKING\n",
      "37. WALKING\n",
      "38. WALKING\n",
      "39. WALKING\n",
      "40. WALKING\n",
      "41. WALKING\n",
      "42. WALKING\n",
      "43. WALKING\n",
      "44. WALKING\n",
      "45. WALKING DOWNSTAIRS\n",
      "46. WALKING\n",
      "47. WALKING\n",
      "48. WALKING\n",
      "49. WALKING\n",
      "50. WALKING DOWNSTAIRS\n",
      "51. WALKING DOWNSTAIRS\n",
      "52. WALKING DOWNSTAIRS\n",
      "53. WALKING DOWNSTAIRS\n"
     ]
    }
   ],
   "source": [
    "query_zero_shot_2 = f\"\"\"\n",
    "* You are a highly accurate activity classification model.\n",
    "* Your task is to classify human activities based on the given accelerometer data.\n",
    "* The accelerometer data is provided as mean acceleration values in the x, y, and z directions.\n",
    "* You are given data corresponding to six different activities.\n",
    "* The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING UPSTAIRS, and WALKING DOWNSTAIRS.\n",
    "* Analyze the accelerometer data and provide the most likely activity label for each case.\n",
    "\n",
    "accx , accy, accz = {test_mean_df[\"accx\"]},{test_mean_df[\"accy\"]},{test_mean_df[\"accz\"]}\n",
    "\n",
    "just print the result of all from zero to 53 th\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Example code to invoke the model\n",
    "response_zero_shot_2 = llm.invoke(query_zero_shot_2)\n",
    "print(\"Zero-Shot Classification:\", response_zero_shot_2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mean_df['Prediction']  =   [\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING DOWNSTAIRS\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING DOWNSTAIRS\", \"WALKING DOWNSTAIRS\", \"WALKING DOWNSTAIRS\", \"WALKING DOWNSTAIRS\"\n",
    "]\n",
    "len(test_mean_df)\n",
    "\n",
    "new_accuracy = (test_mean_df['Activity'] == test_mean_df['Prediction']).mean()\n",
    "new_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this very low is because we pass the mean of all the data and we lost too much data and the acc in x,y,z became constant while it should not be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CASE III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero shot for whole data with the range of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "1\n",
      "WALKING WALKING\n",
      "2\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "3\n",
      "WALKING WALKING\n",
      "4\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "5\n",
      "SITTING WALKING\n",
      "Standing WALKING\n",
      "WALKING WALKING\n",
      "6\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "7\n",
      "WALKING WALKING\n",
      "8\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS STANDING\n",
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "9\n",
      "WALKING WALKING\n",
      "10\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "11\n",
      "WALKING WALKING\n",
      "12\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "13\n",
      "WALKING WALKING\n",
      "14\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "15\n",
      "WALKING WALKING\n",
      "16\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "17\n",
      "WALKING WALKING\n",
      "18\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "33.33333333333333\n",
      "Classification complete. Results have been saved to 'model_predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming you have the test data loaded as a DataFrame\n",
    "test_data_df = pd.read_csv('test_data_combined.csv')\n",
    "\n",
    "# Placeholder for storing results\n",
    "results = []\n",
    "\n",
    "# Define the activities\n",
    "activities = ['LAYING', 'STANDING', 'WALKING', 'SITTING', 'WALKING UPSTAIRS', 'WALKING DOWNSTAIRS']\n",
    "\n",
    "# Function to sample data\n",
    "def sample_data(data, sample_rate=15):\n",
    "    n=500//sample_rate\n",
    "    return data[:n]\n",
    "i=0\n",
    "# Iterate through each group by subject and activity\n",
    "for (subject, activity), group in test_data_df.groupby(['Subject', 'Activity']):\n",
    "    # Sample data\n",
    "    accx = sample_data(group['accx'].tolist())\n",
    "    accy = sample_data(group['accy'].tolist())\n",
    "    accz = sample_data(group['accz'].tolist())\n",
    "    \n",
    "    # Create prompt for the current group\n",
    "    query_zero_shot_3 = f\"\"\"\n",
    "    * You are a highly accurate activity classification model.\n",
    "    * Your task is to classify human activities based on the given accelerometer data.\n",
    "    * The accelerometer data is provided as mean acceleration values in the x, y, and z directions.\n",
    "    * You are given data corresponding to six different activities.\n",
    "    * The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING UPSTAIRS, and WALKING DOWNSTAIRS.\n",
    "    * Analyze the accelerometer data and provide the most likely activity label for each case.\n",
    "    * PRINT ONLY A WORD WHICH IS THE PREDICTED ACTIVITY AND NOTHING ELSE NO CONTAIN NO REASON JUST A PREDICTION\n",
    "\n",
    "    accx = {accx}\n",
    "    accy = {accy}\n",
    "    accz = {accz}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate model prediction (replace with actual model prediction code)\n",
    "    result = llm.invoke(query_zero_shot_3)        \n",
    "        \n",
    "    print(activity,str(result).split(\" \")[0][8:].strip(\"'\"))\n",
    "    if (activity.upper() ==str(result).split(\" \")[0][8:].strip(\"'\")):\n",
    "        i+=1\n",
    "        print(i)\n",
    "\n",
    "    results.append(str(result).split(\" \")[0][8:])\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to CSV if needed\n",
    "results_df.to_csv('model_predictions_zeroshot.csv', index=False)\n",
    "\n",
    "print(i/54*100)\n",
    "\n",
    "print(\"Classification complete. Results have been saved to 'model_predictions.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33.33 PERCENT ACUURACY FOR ZERO-SHOT OVER ALL DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">FEW SHOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CASE I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only 6 data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Classification: Here are the classifications:\n",
      "\n",
      "1. WALKING\n",
      "2. STANDING\n",
      "3. STANDING\n",
      "4. SITTING\n",
      "5. WALKING DOWNSTAIRS\n",
      "6. WALKING DOWNSTAIRS\n"
     ]
    }
   ],
   "source": [
    "query_few_shot = f\"\"\"\n",
    "* You are an activity classification model.\n",
    "* Your task is to classify human activities based on the given accelerometer data.\n",
    "* The accelerometer data is provided as mean acceleration values in the x, y, and z directions.\n",
    "* The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING UPSTAIRS, and WALKING DOWNSTAIRS.\n",
    "\n",
    "Here are some labeled examples:\n",
    "\n",
    "Example 1: \n",
    "Activity: LAYING\n",
    "Accelerometer Data: {Train_sub1_laying.mean()}\n",
    "\n",
    "Example 2: \n",
    "Activity: SITTING\n",
    "Accelerometer Data: {Train_sub3_sitting.mean()}\n",
    "\n",
    "Example 3: \n",
    "Activity: STANDING\n",
    "Accelerometer Data: {Train_sub5_standing.mean()}\n",
    "\n",
    "Example 4: \n",
    "Activity: WALKING\n",
    "Accelerometer Data: {Train_sub7_walking.mean()}\n",
    "\n",
    "Example 5: \n",
    "Activity: WALKING DOWNSTAIRS\n",
    "Accelerometer Data: {Train_sub8_walking_downstairs.mean()}\n",
    "\n",
    "Example 6: \n",
    "Activity: WALKING UPSTAIRS\n",
    "Accelerometer Data: {Train_sub11_walking_upstairs.mean()}\n",
    "\n",
    "Now, based on the provided examples, classify the following new accelerometer data:\n",
    "1. Accelerometer Data:  {Test_sub2_laying.mean()}\n",
    "2. Accelerometer Data:  {Test_sub9_sitting.mean()}\n",
    "3. Accelerometer Data:  {Test_sub10_standing.mean()}\n",
    "4. Accelerometer Data:  {Test_sub4_walking.mean()}\n",
    "5. Accelerometer Data:  {Test_sub20_walking_downstairs.mean()}\n",
    "6. Accelerometer Data:  {Test_sub24_walking_upstairs.mean()}\n",
    "\n",
    "\n",
    "only print the result of output\n",
    "\"\"\"\n",
    "\n",
    "# Example code to invoke the model\n",
    "response_few_shot_2 = llm.invoke(query_few_shot)\n",
    "print(\"Few-Shot Classification:\", response_few_shot_2.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33.33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CASE II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few shot for whole data with the average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Classification: Based on the provided accelerometer data, I will classify the activities as follows:\n",
      "\n",
      "0. LAYING\n",
      "1. LAYING\n",
      "2. LAYING\n",
      "3. LAYING\n",
      "4. LAYING\n",
      "5. LAYING\n",
      "6. LAYING\n",
      "7. LAYING\n",
      "8. LAYING\n",
      "9. WALKING_UPSTAIRS\n",
      "10. WALKING_UPSTAIRS\n",
      "11. WALKING_UPSTAIRS\n",
      "12. WALKING_UPSTAIRS\n",
      "13. WALKING_UPSTAIRS\n",
      "14. WALKING_UPSTAIRS\n",
      "15. WALKING_UPSTAIRS\n",
      "16. WALKING_UPSTAIRS\n",
      "17. WALKING_UPSTAIRS\n",
      "18. WALKING_UPSTAIRS\n",
      "19. WALKING_UPSTAIRS\n",
      "20. WALKING_UPSTAIRS\n",
      "21. WALKING_UPSTAIRS\n",
      "22. WALKING_UPSTAIRS\n",
      "23. WALKING_UPSTAIRS\n",
      "24. WALKING_UPSTAIRS\n",
      "25. WALKING_UPSTAIRS\n",
      "26. WALKING_UPSTAIRS\n",
      "27. WALKING_UPSTAIRS\n",
      "28. WALKING_UPSTAIRS\n",
      "29. WALKING_UPSTAIRS\n",
      "30. WALKING_UPSTAIRS\n",
      "31. WALKING_UPSTAIRS\n",
      "32. WALKING_UPSTAIRS\n",
      "33. WALKING_UPSTAIRS\n",
      "34. WALKING_UPSTAIRS\n",
      "35. WALKING_UPSTAIRS\n",
      "36. WALKING_UPSTAIRS\n",
      "37. WALKING_UPSTAIRS\n",
      "38. WALKING_UPSTAIRS\n",
      "39. WALKING_UPSTAIRS\n",
      "40. WALKING_UPSTAIRS\n",
      "41. WALKING_UPSTAIRS\n",
      "42. WALKING_UPSTAIRS\n",
      "43. WALKING_UPSTAIRS\n",
      "44. WALKING_UPSTAIRS\n",
      "45. WALKING_DOWNSTAIRS\n",
      "46. WALKING_DOWNSTAIRS\n",
      "47. WALKING_DOWNSTAIRS\n",
      "48. WALKING_DOWNSTAIRS\n",
      "49. WALKING_DOWNSTAIRS\n",
      "50. WALKING_DOWNSTAIRS\n",
      "51. WALKING_DOWNSTAIRS\n",
      "52. WALKING_DOWNSTAIRS\n",
      "53. WALKING_DOWNSTAIRS\n",
      "\n",
      "Note that the classification is based on the patterns and characteristics of the accelerometer data, and the results may not be 100% accurate.\n"
     ]
    }
   ],
   "source": [
    "query_few_shot = f\"\"\"\n",
    "* You are an activity classification model.\n",
    "* Your task is to classify human activities based on the given accelerometer data.\n",
    "* The accelerometer data is provided as mean acceleration values in the x, y, and z directions.\n",
    "* The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING UPSTAIRS, and WALKING DOWNSTAIRS.\n",
    "\n",
    "Here are some labeled examples:\n",
    "\n",
    "{train_mean_df}\n",
    "\n",
    "Now, based on the provided examples, classify the following new accelerometer data:\n",
    "predict it with accx , accy, accz = {test_mean_df[\"accx\"]},{test_mean_df[\"accy\"]},{test_mean_df[\"accz\"]}\n",
    "\n",
    "just print the result of all from zero to last\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Example code to invoke the model\n",
    "response_few_shot_2 = llm.invoke(query_few_shot)\n",
    "print(\"Few-Shot Classification:\", response_few_shot_2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mean_df['Prediction']  =  [\n",
    "    \"LAYING\", \"LAYING\", \"LAYING\", \"LAYING\", \"LAYING\",\n",
    "    \"LAYING\", \"LAYING\", \"LAYING\", \"LAYING\", \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\",  \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\", \"WALKING_DOWNSTAIRS\", \"WALKING_DOWNSTAIRS\", \"WALKING_DOWNSTAIRS\", \"WALKING_DOWNSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\", \"WALKING_DOWNSTAIRS\", \"WALKING_DOWNSTAIRS\"\n",
    "]\n",
    "\n",
    "\n",
    "new_accuracy = (test_mean_df['Activity'] == test_mean_df['Prediction']).mean()\n",
    "new_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this very low is because we pass the mean of all the data and we lost too much data and the acc in x,y,z became constant while it should not be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CASE III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few shot for whole data with the range of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYING LAYING\n",
      "1\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "2\n",
      "WALKING WALKING\n",
      "3\n",
      "WALKING_DOWNSTAIRS WALKING_UPSTAIRS\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "4\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "5\n",
      "WALKING WALKING\n",
      "6\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "7\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "8\n",
      "WALKING WALKING\n",
      "9\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "10\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "11\n",
      "WALKING WALKING\n",
      "12\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "13\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "14\n",
      "WALKING WALKING\n",
      "15\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "16\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "17\n",
      "WALKING WALKING\n",
      "18\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "19\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "20\n",
      "WALKING WALKING\n",
      "21\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "22\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "23\n",
      "WALKING WALKING\n",
      "24\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "25\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "26\n",
      "WALKING WALKING\n",
      "27\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "Classification complete. Results have been saved to 'model_predictions.csv'.\n",
      "accuracy 50.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the test data loaded as a DataFrame\n",
    "test_data_df = pd.read_csv('test_data_combined.csv')\n",
    "\n",
    "# Placeholder for storing results\n",
    "results = []\n",
    "\n",
    "# Define the activities\n",
    "activities = ['LAYING', 'STANDING', 'WALKING', 'SITTING', 'WALKING UPSTAIRS', 'WALKING DOWNSTAIRS']\n",
    "\n",
    "def sample_data(data, sample_rate=10):\n",
    "    n=500//sample_rate\n",
    "    return data[:n]\n",
    "\n",
    "i = 0\n",
    "\n",
    "# Filter training data for specific subjects\n",
    "train_subjects = [1, 3, 5, 7]\n",
    "train_data_df = pd.read_csv('train_data_combined.csv')\n",
    "train_data_df = train_data_df[train_data_df['Subject'].isin(train_subjects)]\n",
    "\n",
    "# Iterate through each group by subject and activity\n",
    "for (subject, activity), group in test_data_df.groupby(['Subject', 'Activity']):\n",
    "    # Sample data\n",
    "    accx = sample_data(group['accx'].tolist())\n",
    "    accy = sample_data(group['accy'].tolist())\n",
    "    accz = sample_data(group['accz'].tolist())\n",
    "    \n",
    "    # Create prompt for the current group\n",
    "    query_few_shot_3 = f\"\"\"\n",
    "    * You are a highly accurate activity classification model.\n",
    "    * Your task is to classify human activities based on the given accelerometer data.\n",
    "    * The accelerometer data is provided as mean acceleration values in the x, y, and z directions.\n",
    "    * You are given data corresponding to six different activities.\n",
    "    * The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING UPSTAIRS, and WALKING DOWNSTAIRS.\"\"\"\n",
    "\n",
    "    for (train_subject, train_activity), grp in train_data_df.groupby(['Subject', 'Activity']):\n",
    "        query_few_shot_3 += \"\"\"Here are some examples of accelerometer data and their corresponding activities:\"\"\"\n",
    "\n",
    "        query_few_shot_3 += f\"\"\"\n",
    "        * Activity: {train_activity}\n",
    "          accx = {sample_data(grp['accx'].tolist(), 50)}\n",
    "          accy = {sample_data(grp['accy'].tolist(), 50)}\n",
    "          accz = {sample_data(grp['accz'].tolist(), 50)}\n",
    "        \"\"\"\n",
    "    \n",
    "    query_few_shot_3 += f\"\"\"\n",
    "    * Analyze the accelerometer data and provide the most likely activity label for each case.\n",
    "    * PRINT ONLY A WORD WHICH IS THE PREDICTED ACTIVITY AND NOTHING ELSE NO CONTENT NO REASON JUST A PREDICTION\n",
    "\n",
    "    accx = {accx}\n",
    "    accy = {accy}\n",
    "    accz = {accz}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate model prediction (replace with actual model prediction code)\n",
    "    result = llm.invoke(query_few_shot_3)\n",
    "    \n",
    "    print(activity, str(result).split(\" \")[0][8:].strip(\"'\"))\n",
    "    if (activity.upper() ==str(result).split(\" \")[0][8:].strip(\"'\")):\n",
    "        i+=1\n",
    "        print(i)\n",
    "    results.append({'Subject': subject, 'Activity': activity, 'Prediction': str(result).split(\" \")[0][8:].strip(\"'\")})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to CSV if needed\n",
    "results_df.to_csv('model_predictions_fewshot.csv', index=False)\n",
    "\n",
    "print(\"Classification complete. Results have been saved to 'model_predictions.csv'.\")\n",
    "\n",
    "print(\"accuracy\",i/54*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth\\AppData\\Local\\Temp\\ipykernel_20616\\1879437392.py:10: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  X_features = pd.read_csv(os.path.join(train_path, \"X_train.txt\"), delim_whitespace=True, header=None)\n",
      "C:\\Users\\Parth\\AppData\\Local\\Temp\\ipykernel_20616\\1879437392.py:11: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  featurel = pd.read_csv(os.path.join(train_path, \"..\", \"features.txt\"), delim_whitespace=True, header=None)\n",
      "C:\\Users\\Parth\\AppData\\Local\\Temp\\ipykernel_20616\\1879437392.py:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  y_features = pd.read_csv(os.path.join(train_path, \"y_train.txt\"), delim_whitespace=True, header=None)\n",
      "C:\\Users\\Parth\\AppData\\Local\\Temp\\ipykernel_20616\\1879437392.py:17: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  X_features_test = pd.read_csv(os.path.join(test_path, \"X_test.txt\"), delim_whitespace=True, header=None)\n",
      "C:\\Users\\Parth\\AppData\\Local\\Temp\\ipykernel_20616\\1879437392.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  y_features_test = pd.read_csv(os.path.join(test_path, \"y_test.txt\"), delim_whitespace=True, header=None)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the relative paths\n",
    "train_path = r\"..\\Dataset\\UCI HAR Dataset\\train\"\n",
    "test_path = r\"..\\Dataset\\UCI HAR Dataset\\test\"\n",
    "\n",
    "# Load the training data\n",
    "X_features_test_test_test_test_test_test_test = pd.read_csv(os.path.join(train_path, \"X_train.txt\"), delim_whitespace=True, header=None)\n",
    "featurel = pd.read_csv(os.path.join(train_path, \"..\", \"features.txt\"), delim_whitespace=True, header=None)\n",
    "featurelist = list(featurel.loc[:, 1])\n",
    "X_features_test_test_test_test_test.columns = featurelist\n",
    "y_features = pd.read_csv(os.path.join(train_path, \"y_train.txt\"), delim_whitespace=True, header=None)\n",
    "\n",
    "# Load the test data\n",
    "X_features_test = pd.read_csv(os.path.join(test_path, \"X_test.txt\"), delim_whitespace=True, header=None)\n",
    "X_features_test.columns = featurelist\n",
    "y_features_test = pd.read_csv(os.path.join(test_path, \"y_test.txt\"), delim_whitespace=True, header=None)\n",
    "\n",
    "# Now you can proceed with your analysis or model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tsfel\n",
      "  Downloading tsfel-0.1.8-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: ipython>=7.4.0 in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from tsfel) (8.20.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tsfel) (1.26.3)\n",
      "Requirement already satisfied: pandas>=1.5.3 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tsfel) (2.2.0)\n",
      "Collecting PyWavelets>=1.4.1 (from tsfel)\n",
      "  Downloading pywavelets-1.7.0-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tsfel) (2.31.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tsfel) (1.5.1)\n",
      "Requirement already satisfied: scipy>=1.7.3 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tsfel) (1.13.0)\n",
      "Requirement already satisfied: setuptools>=47.1.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tsfel) (65.5.0)\n",
      "Collecting statsmodels>=0.12.0 (from tsfel)\n",
      "  Downloading statsmodels-0.14.2-cp311-cp311-win_amd64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.4.0->tsfel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.4.0->tsfel) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.4.0->tsfel) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.4.0->tsfel) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.4.0->tsfel) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.4.0->tsfel) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.4.0->tsfel) (5.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.4.0->tsfel) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.5.3->tsfel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.5.3->tsfel) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.5.3->tsfel) (2023.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.31.0->tsfel) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.31.0->tsfel) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.31.0->tsfel) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.31.0->tsfel) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.21.3->tsfel) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.21.3->tsfel) (3.5.0)\n",
      "Collecting patsy>=0.5.6 (from statsmodels>=0.12.0->tsfel)\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from statsmodels>=0.12.0->tsfel) (23.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->ipython>=7.4.0->tsfel) (0.8.3)\n",
      "Requirement already satisfied: six in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from patsy>=0.5.6->statsmodels>=0.12.0->tsfel) (1.16.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.4.0->tsfel) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=7.4.0->tsfel) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=7.4.0->tsfel) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=7.4.0->tsfel) (0.2.2)\n",
      "Downloading tsfel-0.1.8-py3-none-any.whl (61 kB)\n",
      "   ---------------------------------------- 0.0/61.1 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 30.7/61.1 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 30.7/61.1 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 51.2/61.1 kB 375.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 61.1/61.1 kB 406.5 kB/s eta 0:00:00\n",
      "Downloading pywavelets-1.7.0-cp311-cp311-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/4.3 MB 1.7 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.1/4.3 MB 980.4 kB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.1/4.3 MB 983.0 kB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.2/4.3 MB 926.0 kB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.2/4.3 MB 935.2 kB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.2/4.3 MB 838.1 kB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.3/4.3 MB 850.6 kB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.3/4.3 MB 846.5 kB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.4/4.3 MB 857.5 kB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.4/4.3 MB 851.5 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.4/4.3 MB 860.2 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.5/4.3 MB 891.2 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.5/4.3 MB 896.4 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.6/4.3 MB 889.8 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.6/4.3 MB 907.9 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.7/4.3 MB 901.1 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.7/4.3 MB 930.9 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.8/4.3 MB 922.1 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.8/4.3 MB 947.9 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.9/4.3 MB 927.9 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.9/4.3 MB 951.1 kB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.0/4.3 MB 963.4 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 1.1/4.3 MB 992.2 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 1.1/4.3 MB 992.1 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 1.2/4.3 MB 978.6 kB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.2/4.3 MB 995.9 kB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.3/4.3 MB 994.9 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 1.3/4.3 MB 994.8 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 1.4/4.3 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 1.4/4.3 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 1.5/4.3 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 1.5/4.3 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 1.6/4.3 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 1.6/4.3 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 1.7/4.3 MB 1.0 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 1.7/4.3 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 1.8/4.3 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 1.8/4.3 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.9/4.3 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 2.0/4.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 2.0/4.3 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 2.1/4.3 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 2.1/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 2.2/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 2.2/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 2.3/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.4/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.4/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 2.4/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 2.5/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.6/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.6/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 2.7/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 2.7/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 2.8/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 2.8/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.9/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.9/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 3.0/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 3.0/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 3.1/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 3.2/4.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 3.2/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 3.3/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.3/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.4/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.4/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.5/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.6/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.6/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.7/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.7/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.8/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.8/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.9/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.9/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 4.0/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 4.0/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 4.1/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 4.1/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.2/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.2/4.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading statsmodels-0.14.2-cp311-cp311-win_amd64.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.9 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------------------------------- 0.1/9.9 MB 2.2 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.3/9.9 MB 2.4 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/9.9 MB 2.0 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.4/9.9 MB 1.9 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.4/9.9 MB 1.6 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.5/9.9 MB 1.6 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.5/9.9 MB 1.5 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.6/9.9 MB 1.5 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.7/9.9 MB 1.5 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.7/9.9 MB 1.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.7/9.9 MB 1.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.8/9.9 MB 1.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.9/9.9 MB 1.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.9/9.9 MB 1.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.0/9.9 MB 1.4 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.0/9.9 MB 1.3 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.1/9.9 MB 1.3 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.1/9.9 MB 1.3 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.2/9.9 MB 1.3 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.2/9.9 MB 1.3 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.3/9.9 MB 1.3 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.3/9.9 MB 1.3 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.4/9.9 MB 1.3 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.5/9.9 MB 1.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.5/9.9 MB 1.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.6/9.9 MB 1.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.6/9.9 MB 1.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.7/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 1.7/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 1.8/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 1.9/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 1.9/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.0/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.0/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.1/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.1/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.2/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.2/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.3/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.3/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.4/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.4/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 2.5/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 2.5/9.9 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 2.6/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 2.6/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 2.7/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 2.8/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 2.8/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 2.8/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 2.9/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.0/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.0/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.1/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.1/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.2/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 3.3/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 3.3/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 3.4/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 3.4/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 3.5/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 3.5/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 3.6/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 3.6/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 3.7/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 3.7/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 3.8/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 3.9/9.9 MB 1.2 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 3.9/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 3.9/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 4.0/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 4.0/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 4.1/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 4.1/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 4.2/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 4.3/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 4.3/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 4.4/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 4.4/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 4.5/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 4.5/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 4.6/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 4.6/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 4.7/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 4.8/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 4.8/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 4.9/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 4.9/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 5.0/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 5.0/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 5.1/9.9 MB 1.2 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 5.1/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.2/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 5.2/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 5.3/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 5.4/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 5.4/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 5.5/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 5.5/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 5.6/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 5.6/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 5.7/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 5.7/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 5.8/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 5.8/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 5.9/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 5.9/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 6.0/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 6.0/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 6.1/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 6.1/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 6.2/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 6.3/9.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 6.3/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 6.4/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 6.4/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 6.5/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 6.5/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 6.6/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 6.6/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 6.7/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 6.8/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 6.8/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 6.9/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 6.9/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 7.0/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 7.0/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 7.1/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 7.1/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 7.2/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 7.2/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 7.3/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 7.3/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 7.4/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 7.4/9.9 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 7.5/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.5/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.6/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.7/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.7/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.8/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.8/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.9/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.0/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.0/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.1/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.1/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.2/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.2/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.3/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.3/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 8.4/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 8.4/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 8.5/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 8.5/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 8.6/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.6/9.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.9/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.9/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.0/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.0/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.1/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.1/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.2/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.4/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.5/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.5/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.6/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.8/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.8/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/9.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "   ---------------------------------------- 0.0/233.9 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 41.0/233.9 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 81.9/233.9 kB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 143.4/233.9 kB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 174.1/233.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 233.9/233.9 kB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: PyWavelets, patsy, statsmodels, tsfel\n",
      "Successfully installed PyWavelets-1.7.0 patsy-0.5.6 statsmodels-0.14.2 tsfel-0.1.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tsfel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tsfel\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "# # Import data from MakeDataset\n",
    "# from Dataset.MakeDataset import X_train, X_test, y_train, y_test\n",
    "\n",
    "# # Dictionary of activities. Provided by the dataset.\n",
    "# ACTIVITIES = {\n",
    "#     1: 'WALKING'            ,\n",
    "#     2: 'WALKING_UPSTAIRS'   ,\n",
    "#     3: 'WALKING_DOWNSTAIRS' ,\n",
    "#     4: 'SITTING'            ,\n",
    "#     5: 'STANDING'           ,\n",
    "#     6: 'LAYING'             ,\n",
    "# }\n",
    "\n",
    "# # Step 1: Feature Extraction\n",
    "# cfg_file, fs = tsfel.get_features_by_domain(), 50\n",
    "# X_features = tsfel.time_series_features_extractor(cfg_file, X_test, fs=fs)\n",
    "# X_features_features = list(X_features)\n",
    "\n",
    "# # Remove highly correlated features\n",
    "# corr_features = tsfel.correlated_features(X_features)\n",
    "# X_features.drop(corr_features, axis=1, inplace=True)\n",
    "\n",
    "# # Remove low variance features\n",
    "# selector = VarianceThreshold()\n",
    "# X_features = selector.fit_transform(X_features)\n",
    "\n",
    "# # Normalize features\n",
    "# scaler = StandardScaler()\n",
    "# X_normalized = scaler.fit_transform(X_features)\n",
    "\n",
    "# # Step 3: PCA for Dimensionality Reduction\n",
    "# pca = PCA(n_components=2)\n",
    "# X_pca = pca.fit_transform(X_normalized)\n",
    "\n",
    "# # Create a DataFrame for easier plotting\n",
    "# df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])   \n",
    "# df_pca_f = df_pca\n",
    "# df_pca['Activity'] = [ACTIVITIES[activity] for activity in y_test]\n",
    "# display(df_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 PERCENT ACCURACY OF FEW-SHOT OVERALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">A1)\n",
    " Few shots are better it is give examples and has a idea of how the inputs will be for a particular activity while the zero-shot only has generic information nothing in particular.\n",
    "\n",
    ">A2)\n",
    " decision trees accuracy was  61.1% max(from task 2) while the accuracy of the few-shots is 46.3% so the decision is better because it sets up a threshold value and froms more structed value than that in few-shots. and we were also able to give much more data  in the decision tree we may recive closer accuracy it the number of token would have not been limited.\n",
    "\n",
    ">A3) \n",
    "few shot depends a lot on the exmaples given in it so it should be given a different varity of samples other wise it will be overfitted on the data given and will have high bais. and both zero shot and few shot struggles for new data which they have not seen before.\n",
    "\n",
    ">A4)\n",
    " few shot is more likely to predict it as the data which is seen unless there is significant difference between the new unseen sample and the train data set. In zero shot there is not train data so it will predict it to the closes activity possible based on generic information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">A5)\n",
    " most of the result shown are dynamic as I did unifrom distribution over -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       accx      accy      accz\n",
      "0 -0.600466 -0.424734 -0.046532\n",
      "1 -0.538894 -0.496867 -0.358970\n",
      "2  0.478016 -0.793912 -0.274391\n",
      "3  0.636616 -0.083124  0.131175\n",
      "4  0.860025  0.155066  0.793801\n",
      "Prediction for the random data: ='WALKING_UPSTAIRS'\n",
      "       accx      accy      accz\n",
      "0 -0.291719  0.863479  0.004827\n",
      "1  0.129610 -0.401998 -0.090099\n",
      "2  0.038629 -0.431866 -0.924246\n",
      "3  0.204572  0.147227 -0.650012\n",
      "4 -0.135920 -0.535460 -0.043527\n",
      "Prediction for the random data: ='WALKING'\n",
      "       accx      accy      accz\n",
      "0 -0.018285 -0.323576  0.476706\n",
      "1 -0.967154  0.013055  0.510384\n",
      "2  0.952484  0.288089 -0.736565\n",
      "3 -0.729330 -0.305223  0.949006\n",
      "4  0.531035  0.232746 -0.632187\n",
      "Prediction for the random data: ='WALKING'\n",
      "       accx      accy      accz\n",
      "0  0.515219 -0.347614 -0.654486\n",
      "1  0.570632  0.110917  0.028251\n",
      "2  0.903048 -0.649447  0.419353\n",
      "3 -0.435997 -0.682641  0.521948\n",
      "4  0.436086 -0.986751  0.886106\n",
      "Prediction for the random data: ='WALKING'\n",
      "       accx      accy      accz\n",
      "0 -0.549071  0.883861  0.211522\n",
      "1  0.493467  0.469254 -0.869282\n",
      "2 -0.327979 -0.820570  0.060077\n",
      "3 -0.681408 -0.100256  0.742978\n",
      "4 -0.127257  0.652142 -0.948329\n",
      "Prediction for the random data: ='WALKING_UPSTAIRS'\n",
      "       accx      accy      accz\n",
      "0 -0.722593  0.608537 -0.909785\n",
      "1 -0.307464 -0.436524 -0.601029\n",
      "2  0.510171 -0.338077  0.224081\n",
      "3  0.049980  0.306230  0.126814\n",
      "4 -0.889181  0.538261 -0.553405\n",
      "Prediction for the random data: ='WALKING'\n",
      "       accx      accy      accz\n",
      "0 -0.308523  0.375486  0.633201\n",
      "1  0.507949 -0.374635  0.052467\n",
      "2  0.862335  0.453301 -0.285743\n",
      "3 -0.519897  0.520324  0.319997\n",
      "4  0.805536 -0.706473  0.001633\n",
      "Prediction for the random data: ='WALKING_UPSTAIRS'\n",
      "       accx      accy      accz\n",
      "0  0.148284 -0.636446  0.673779\n",
      "1  0.459367  0.715010 -0.779902\n",
      "2  0.436643 -0.187411 -0.885731\n",
      "3  0.180823 -0.839379  0.343762\n",
      "4  0.606678  0.714484 -0.008213\n",
      "Prediction for the random data: ='WALKING_UPSTAIRS'\n",
      "       accx      accy      accz\n",
      "0  0.218621  0.184639 -0.021026\n",
      "1  0.382638 -0.091631 -0.055060\n",
      "2 -0.543091 -0.197608  0.188010\n",
      "3 -0.545442  0.299030 -0.984168\n",
      "4  0.180162 -0.857444 -0.393621\n",
      "Prediction for the random data: ='WALKING_UPSTAIRS'\n",
      "       accx      accy      accz\n",
      "0 -0.803738  0.902307 -0.082061\n",
      "1  0.594994 -0.218938 -0.096130\n",
      "2  0.483094 -0.016742  0.969093\n",
      "3 -0.557641 -0.806719 -0.801226\n",
      "4  0.125700  0.396533  0.017753\n",
      "Prediction for the random data: ='WALKING'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "for _ in range(10):\n",
    "    # Generate random accelerometer data with the same dimensions and range as typical input\n",
    "    num_samples = 500\n",
    "    accx = np.random.uniform(-1, 1, num_samples)\n",
    "    accy = np.random.uniform(-1, 1, num_samples)\n",
    "    accz = np.random.uniform(-1, 1, num_samples)\n",
    "\n",
    "    # Create a DataFrame to represent the random data\n",
    "    random_data = pd.DataFrame({\n",
    "        'accx': accx,\n",
    "        'accy': accy,\n",
    "        'accz': accz\n",
    "    })\n",
    "\n",
    "    # Display the first few rows of the random data\n",
    "    print(random_data.head())\n",
    "\n",
    "    # Function to sample data\n",
    "    def sample_data(data, sample_rate=10):\n",
    "        return data[::sample_rate]\n",
    "\n",
    "    # Create a Few-Shot Learning prompt\n",
    "    query_few_shot_3 = f\"\"\"\n",
    "    * You are a highly accurate activity classification model.\n",
    "    * Your task is to classify human activities based on the given accelerometer data.\n",
    "    * The accelerometer data is provided as mean acceleration values in the x, y, and z directions.\n",
    "    * You are given data corresponding to six different activities.\n",
    "    * The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING UPSTAIRS, and WALKING DOWNSTAIRS.\"\"\"\n",
    "\n",
    "    train_data_df = pd.read_csv('train_data_combined.csv')\n",
    "    train_subjects = [1, 3, 5, 7]\n",
    "    train_data_df = train_data_df[train_data_df['Subject'].isin(train_subjects)]\n",
    "\n",
    "    for (train_subject, train_activity), grp in train_data_df.groupby(['Subject', 'Activity']):\n",
    "        query_few_shot_3 += \"\"\"Here are some examples of accelerometer data and their corresponding activities:\"\"\"\n",
    "        query_few_shot_3 += f\"\"\"\n",
    "        * Activity: {train_activity}\n",
    "        accx = {sample_data(grp['accx'].tolist(), 50)}\n",
    "        accy = {sample_data(grp['accy'].tolist(), 50)}\n",
    "        accz = {sample_data(grp['accz'].tolist(), 50)}\n",
    "        \"\"\"\n",
    "\n",
    "    # Add the random accelerometer data to the prompt\n",
    "    query_few_shot_3 += f\"\"\"\n",
    "    * Analyze the accelerometer data and provide the most likely activity label for this case.\n",
    "    * PRINT ONLY A WORD WHICH IS THE PREDICTED ACTIVITY AND NOTHING ELSE NO CONTENT NO REASON JUST A PREDICTION\n",
    "\n",
    "    accx = {sample_data(random_data['accx'].tolist(), 10)}\n",
    "    accy = {sample_data(random_data['accy'].tolist(), 10)}\n",
    "    accz = {sample_data(random_data['accz'].tolist(), 10)}\n",
    "    \"\"\"\n",
    "\n",
    "    # Simulate model prediction (replace with actual model prediction code)\n",
    "    result = llm.invoke(query_few_shot_3)\n",
    "\n",
    "    print(\"Prediction for the random data:\", str(result).split(\" \")[0][7:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
